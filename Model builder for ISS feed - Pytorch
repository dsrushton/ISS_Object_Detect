import os
import torch
import pandas as pd
from PIL import Image
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision.transforms as transforms
import torchvision.models.detection as detection
import torch.optim as optim
from tqdm import tqdm

# Define the class mapping
class_mapping = {
    'Space': 1,
    'Earth_broad': 2,
    'ISS': 3,
    'panels': 4,
    'e_t': 5,
    'no_feed': 6,
    't_d': 7,
    'lf': 8,
    'unknown': 9,
    'sun': 0
}

# Define the Dataset class for bounding boxes
class BoundingBoxDataset(Dataset):
    def __init__(self, directories, transform=None, class_mapping=None):
        self.directories = directories
        self.image_paths = []
        self.bbox_paths = []
        self.transform = transform
        self.class_mapping = class_mapping if class_mapping else {}

        for directory in directories:
            images = sorted([f for f in os.listdir(directory) if f.endswith('.jpg')])
            for img_name in images:
                img_path = os.path.join(directory, img_name)
                bbox_path = os.path.join(directory, f"bboxes_{img_name.split('_')[-1].split('.')[0]}.csv")
                self.image_paths.append(img_path)
                self.bbox_paths.append(bbox_path)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        bbox_csv_path = self.bbox_paths[idx]
        
        image = Image.open(img_path).convert("RGB")
        bboxes = pd.read_csv(bbox_csv_path)
        
        bboxes['xmax'] = bboxes['x'] + bboxes['width']
        bboxes['ymax'] = bboxes['y'] + bboxes['height']
        bboxes['class_id'] = bboxes['class'].map(self.class_mapping)

        boxes = bboxes[['x', 'y', 'xmax', 'ymax']].values.astype(np.float32)
        labels = bboxes['class_id'].values.astype(np.int64)

        target = {"boxes": torch.tensor(boxes, dtype=torch.float32),
                  "labels": torch.tensor(labels, dtype=torch.int64)}
        
        if self.transform:
            image = self.transform(image)
        
        return image, target

# Parameters
directories = [
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_1/frames_1",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_1/frames_5",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_3/frames_1",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_4_getting_dark/frames_1",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_5_earth_small_space_patch/frames_1",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_6_clean_darkness/frames_1",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_7_overflight/frames_1",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_8_darkness_on_iss/frames_1",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_9/frames_3",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_12/frames_1",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_13_dark_flyover/frames_1",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/sample_18_sunrise/frames_5",
    "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/badsample_test2/frames_1"
]
working_dir = "C:/Users/dsrus/Desktop/Workspace/UAP_Python_2/train23august_bb"
batch_size = 4
num_epochs = 10
split_ratio = 0.8

# Define transforms
transform = transforms.Compose([
    transforms.ToTensor(),
])

# Load dataset and split into training and validation sets
dataset = BoundingBoxDataset(directories=directories, transform=transform, class_mapping=class_mapping)
train_size = int(split_ratio * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))

# Initialize the model (Faster R-CNN)
model = detection.fasterrcnn_resnet50_fpn(pretrained=True)
num_classes = len(class_mapping) + 1
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)

model = model.cuda()

# Define optimizer and learning rate
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Function to calculate accuracy
def calculate_accuracy(outputs, targets):
    correct_boxes = 0
    total_boxes = 0
    
    for output, target in zip(outputs, targets):
        pred_boxes = output['boxes'].detach().cpu()
        pred_labels = output['labels'].detach().cpu()
        true_boxes = target['boxes'].cpu()
        true_labels = target['labels'].cpu()

        # Compare predicted and true labels and boxes
        for i in range(len(pred_boxes)):
            if pred_labels[i] in true_labels:
                correct_boxes += 1
            total_boxes += 1
    
    return correct_boxes / total_boxes if total_boxes > 0 else 0

# Training loop with best model saving
best_val_accuracy = 0.0

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    
    with tqdm(total=len(train_loader), desc=f"Epoch [{epoch+1}/{num_epochs}]", unit="batch") as pbar:
        for images, targets in train_loader:
            images = [image.cuda() for image in images]
            targets = [{k: v.cuda() for k, v in t.items()} for t in targets]
            
            optimizer.zero_grad()
            loss_dict = model(images, targets)
            losses = sum(loss for loss in loss_dict.values())
            losses.backward()
            optimizer.step()
            
            running_loss += losses.item()
            pbar.set_postfix({"Loss": losses.item()})
            pbar.update(1)
    
    avg_loss = running_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")
    
    # Validation step
    model.eval()
    val_running_loss = 0.0
    val_running_accuracy = 0.0
    
    with torch.no_grad():
        for images, targets in val_loader:
            images = [image.cuda() for image in images]
            targets = [{k: v.cuda() for k, v in t.items()} for t in targets]
            
            # In validation, we only forward pass images to get predictions
            outputs = model(images)
            
            # Compute accuracy based on outputs and targets
            batch_accuracy = calculate_accuracy(outputs, targets)
            val_running_accuracy += batch_accuracy
            
            # Skipping loss calculation during validation since it's returning predictions
            # If you want to calculate validation loss, provide targets to the model during validation
            
    val_avg_accuracy = val_running_accuracy / len(val_loader)
    print(f"Validation Accuracy: {val_avg_accuracy:.4f}")
    
    # Save the model if validation accuracy improves
    if val_avg_accuracy > best_val_accuracy:
        best_val_accuracy = val_avg_accuracy
        model_save_path = os.path.join(working_dir, "best_faster_rcnn_model.pth")
        torch.save(model.state_dict(), model_save_path)
        print(f"New best model saved to {model_save_path} with Validation Accuracy: {val_avg_accuracy:.4f}")

# Final save
final_model_save_path = os.path.join(working_dir, "final_faster_rcnn_model.pth")
torch.save(model.state_dict(), final_model_save_path)
print(f"Final model saved to {final_model_save_path}")
